{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "015ea033-ede9-4a4d-8a9f-6ab6a4c128f3",
   "metadata": {},
   "source": [
    "# GameFace\n",
    "HCI With Head Pose Using Computer Vision\n",
    "\n",
    "---\n",
    "Practice Module: Intelligent Sensing System (ISS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e45cd-b361-420f-9838-75b6abd59a05",
   "metadata": {},
   "source": [
    "# 0. File Path & Library Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf917b8c-7235-48f8-8d98-60df858eb55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions of key libraries\n",
      "-------------------------\n",
      "pandas:       1.3.2\n",
      "numpy:        1.21.2\n",
      "opencv:       4.5.4-dev\n",
      "tensorflow:   2.7.0\n",
      "GPU Accress   [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Load All Necessary Packages\n",
    "\n",
    "import os\n",
    "# from google.colab import drive\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from math import ceil\n",
    "import math\n",
    "import winsound\n",
    "import webbrowser\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "from scipy.spatial.transform import Rotation\n",
    "from IPython.display import clear_output\n",
    "from math import cos\n",
    "from math import sin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.nn import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "seed = 12\n",
    "\n",
    "print(\"Versions of key libraries\")\n",
    "print(\"-------------------------\")\n",
    "print(\"pandas:      \", pd.__version__)\n",
    "print(\"numpy:       \", np.__version__)\n",
    "print(\"opencv:      \", cv2.__version__)\n",
    "print(\"tensorflow:  \", tf.__version__)\n",
    "print(\"GPU Accress  \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3851193-54cc-427e-b05f-3030ff0a3548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Kennedy\\\\OneDrive\\\\Documents\\\\NUS\\\\Projects\\\\ISSM\\\\GIT\\\\NUS_ISS_ITSS_Project_GameFace\\\\Experimentation\\\\HeadPose_mouth_and_eyes_combined'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Mounting to Google Drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# # Change Working Directory\n",
    "# os.chdir('/content/gdrive/My Drive/iss/prs_pm/training')\n",
    "\n",
    "print('Working Directory: ')\n",
    "# !pwd\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6ab18-0c4d-427c-a749-c8671b4c9112",
   "metadata": {},
   "source": [
    "# 1. Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555be1b-e31e-4c43-82b1-684f0f422477",
   "metadata": {},
   "source": [
    "## Ultralight Face Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cdf62f6-a100-43e4-a041-d5f1cbb4f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "caffemodel = '..\\\\models_and_weights\\\\Slim-320\\\\slim-320.caffemodel'\n",
    "prototxt = '..\\\\models_and_weights\\\\Slim-320\\\\slim-320.prototxt'\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt, caffemodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e4fa2-e961-4e57-b947-3add79197798",
   "metadata": {},
   "source": [
    "## Head Pose Estimation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c6719a-ca8f-4e82-a8d3-72946907fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = os.path.join(os.getcwd(), '..//models_and_weights//HPN_EfficientNetB0_FT_16lyr_v2')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "hpn = load_model(model_file, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ad7d19-25a1-4bf7-ab45-9ffb4b70218a",
   "metadata": {},
   "source": [
    "# 2. Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680717f3-5893-4458-a1fb-26db3672961f",
   "metadata": {},
   "source": [
    "## Face Detection Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ab6c64-d8bd-4033-8f5b-14799f5d8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mean = np.array([127, 127, 127])\n",
    "image_std = 128.0\n",
    "iou_threshold = 0.3\n",
    "center_variance = 0.1\n",
    "size_variance = 0.2\n",
    "min_boxes = [[10.0, 16.0, 24.0], [32.0, 48.0], [64.0, 96.0], [128.0, 192.0, 256.0]]\n",
    "strides = [8.0, 16.0, 32.0, 64.0]\n",
    "\n",
    "\n",
    "def define_img_size(image_size):\n",
    "    shrinkage_list = []\n",
    "    feature_map_w_h_list = []\n",
    "    for size in image_size:\n",
    "        feature_map = [int(ceil(size / stride)) for stride in strides]\n",
    "        feature_map_w_h_list.append(feature_map)\n",
    "\n",
    "    for i in range(0, len(image_size)):\n",
    "        shrinkage_list.append(strides)\n",
    "    priors = generate_priors(feature_map_w_h_list, shrinkage_list, image_size, min_boxes)\n",
    "    return priors\n",
    "\n",
    "\n",
    "def generate_priors(feature_map_list, shrinkage_list, image_size, min_boxes):\n",
    "    priors = []\n",
    "    for index in range(0, len(feature_map_list[0])):\n",
    "        scale_w = image_size[0] / shrinkage_list[0][index]\n",
    "        scale_h = image_size[1] / shrinkage_list[1][index]\n",
    "        for j in range(0, feature_map_list[1][index]):\n",
    "            for i in range(0, feature_map_list[0][index]):\n",
    "                x_center = (i + 0.5) / scale_w\n",
    "                y_center = (j + 0.5) / scale_h\n",
    "\n",
    "                for min_box in min_boxes[index]:\n",
    "                    w = min_box / image_size[0]\n",
    "                    h = min_box / image_size[1]\n",
    "                    priors.append([\n",
    "                        x_center,\n",
    "                        y_center,\n",
    "                        w,\n",
    "                        h\n",
    "                    ])\n",
    "    # print(\"priors nums:{}\".format(len(priors)))\n",
    "    return np.clip(priors, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n",
    "    scores = box_scores[:, -1]\n",
    "    boxes = box_scores[:, :-1]\n",
    "    picked = []\n",
    "    indexes = np.argsort(scores)\n",
    "    indexes = indexes[-candidate_size:]\n",
    "    while len(indexes) > 0:\n",
    "        current = indexes[-1]\n",
    "        picked.append(current)\n",
    "        if 0 < top_k == len(picked) or len(indexes) == 1:\n",
    "            break\n",
    "        current_box = boxes[current, :]\n",
    "        indexes = indexes[:-1]\n",
    "        rest_boxes = boxes[indexes, :]\n",
    "        iou = iou_of(\n",
    "            rest_boxes,\n",
    "            np.expand_dims(current_box, axis=0),\n",
    "        )\n",
    "        indexes = indexes[iou <= iou_threshold]\n",
    "    return box_scores[picked, :]\n",
    "\n",
    "\n",
    "def area_of(left_top, right_bottom):\n",
    "    hw = np.clip(right_bottom - left_top, 0.0, None)\n",
    "    return hw[..., 0] * hw[..., 1]\n",
    "\n",
    "\n",
    "def iou_of(boxes0, boxes1, eps=1e-5):\n",
    "    overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n",
    "    overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:])\n",
    "\n",
    "    overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n",
    "    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n",
    "    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n",
    "    return overlap_area / (area0 + area1 - overlap_area + eps)\n",
    "\n",
    "\n",
    "def predict(width, height, confidences, boxes, prob_threshold, iou_threshold=0.3, top_k=-1):\n",
    "    boxes = boxes[0]\n",
    "    confidences = confidences[0]\n",
    "    picked_box_probs = []\n",
    "    picked_labels = []\n",
    "    for class_index in range(1, confidences.shape[1]):\n",
    "        probs = confidences[:, class_index]\n",
    "        mask = probs > prob_threshold\n",
    "        probs = probs[mask]\n",
    "        if probs.shape[0] == 0:\n",
    "            continue\n",
    "        subset_boxes = boxes[mask, :]\n",
    "        box_probs = np.concatenate([subset_boxes, probs.reshape(-1, 1)], axis=1)\n",
    "        box_probs = hard_nms(box_probs,\n",
    "                             iou_threshold=iou_threshold,\n",
    "                             top_k=top_k,\n",
    "                             )\n",
    "        picked_box_probs.append(box_probs)\n",
    "        picked_labels.extend([class_index] * box_probs.shape[0])\n",
    "    if not picked_box_probs:\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "    picked_box_probs = np.concatenate(picked_box_probs)\n",
    "    picked_box_probs[:, 0] *= width\n",
    "    picked_box_probs[:, 1] *= height\n",
    "    picked_box_probs[:, 2] *= width\n",
    "    picked_box_probs[:, 3] *= height\n",
    "    return picked_box_probs[:, :4].astype(np.int32), np.array(picked_labels), picked_box_probs[:, 4]\n",
    "\n",
    "\n",
    "def convert_locations_to_boxes(locations, priors, center_variance,\n",
    "                               size_variance):\n",
    "    if len(priors.shape) + 1 == len(locations.shape):\n",
    "        priors = np.expand_dims(priors, 0)\n",
    "    return np.concatenate([\n",
    "        locations[..., :2] * center_variance * priors[..., 2:] + priors[..., :2],\n",
    "        np.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n",
    "    ], axis=len(locations.shape) - 1)\n",
    "\n",
    "\n",
    "def center_form_to_corner_form(locations):\n",
    "    return np.concatenate([locations[..., :2] - locations[..., 2:] / 2,\n",
    "                           locations[..., :2] + locations[..., 2:] / 2], len(locations.shape) - 1)\n",
    "\n",
    "def get_bbox(rawimg):\n",
    "    priors = define_img_size([input_width, input_height])\n",
    "    img = cv2.resize(rawimg, (input_width, input_height))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    blob = cv2.dnn.blobFromImage(img, 1/input_std, (input_width, input_height), 127)\n",
    "    net.setInput(blob)\n",
    "    boxes, scores = net.forward([\"boxes\", \"scores\"])\n",
    "    boxes = np.expand_dims(np.reshape(boxes, (-1, 4)), axis=0)\n",
    "    scores = np.expand_dims(np.reshape(scores, (-1, 2)), axis=0)\n",
    "    boxes = convert_locations_to_boxes(boxes, priors, center_variance, size_variance)\n",
    "    boxes = center_form_to_corner_form(boxes)\n",
    "    boxes, labels, probs = predict(rawimg.shape[1], rawimg.shape[0], scores, boxes, threshold)\n",
    "    return boxes\n",
    "\n",
    "def get_best_bbox(boxes, rawimg_shape):\n",
    "    img_center = np.array([rawimg_shape[0], rawimg_shape[1]])/2\n",
    "    center_diff = []\n",
    "    for i in range(len(boxes)):\n",
    "        box_x = (boxes[i][2] - boxes[i][0]) / 2 + boxes[i][0]\n",
    "        box_y = (boxes[i][3] - boxes[i][1]) / 2 + boxes[i][1]\n",
    "        box_center = np.array([box_x, box_y])\n",
    "        euc_dist = np.linalg.norm(box_center - img_center)\n",
    "        center_diff.append(euc_dist)\n",
    "    best_box_idx = np.argmin(np.array(center_diff))\n",
    "    mid_box = boxes[best_box_idx]\n",
    "    return mid_box\n",
    "\n",
    "def get_euler(anno_txt):\n",
    "    file = open(anno_txt)\n",
    "    lines = file.readlines()\n",
    "    R = []\n",
    "    for i in range(3):\n",
    "        line = lines[i]\n",
    "        line = line.rstrip().split(' ')\n",
    "        R.append(np.array(line, dtype=np.float))\n",
    "    R = np.transpose(R)\n",
    "    pitch = math.atan2(R[2,1] , R[2,2])* 180 / np.pi\n",
    "    yaw = -math.atan2(-R[2,0], math.sqrt(R[2,1]**2 + R[2,2]**2))* 180 / np.pi\n",
    "    roll = -math.atan2(R[1,0], R[0,0])* 180 / np.pi\n",
    "    return np.array([pitch, yaw, roll], dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4d7c15-593c-4dbf-9fb1-7fbff5a1f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Euler Axis\n",
    "\n",
    "def draw_axis(img, yaw, pitch, roll, tdx=None, tdy=None, size = 100):\n",
    "\n",
    "    pitch = pitch * np.pi / 180\n",
    "    yaw = -(yaw * np.pi / 180)\n",
    "    roll = roll * np.pi / 180\n",
    "\n",
    "    if tdx != None and tdy != None:\n",
    "        tdx = tdx\n",
    "        tdy = tdy\n",
    "    else:\n",
    "        height, width = img.shape[:2]\n",
    "        tdx = width / 2\n",
    "        tdy = height / 2\n",
    "\n",
    "    # X-Axis pointing to right. drawn in red\n",
    "    x1 = size * (cos(yaw) * cos(roll)) + tdx\n",
    "    y1 = size * (cos(pitch) * sin(roll) + cos(roll) * sin(pitch) * sin(yaw)) + tdy\n",
    "\n",
    "    # Y-Axis | drawn in green\n",
    "    #        v\n",
    "    x2 = size * (-cos(yaw) * sin(roll)) + tdx\n",
    "    y2 = size * (cos(pitch) * cos(roll) - sin(pitch) * sin(yaw) * sin(roll)) + tdy\n",
    "\n",
    "    # Z-Axis (out of the screen) drawn in blue\n",
    "    x3 = size * (sin(yaw)) + tdx\n",
    "    y3 = size * (-cos(yaw) * sin(pitch)) + tdy\n",
    "\n",
    "    cv2.line(img, (int(tdx), int(tdy)), (int(x1),int(y1)),(0,0,255),3)\n",
    "    cv2.line(img, (int(tdx), int(tdy)), (int(x2),int(y2)),(0,255,0),3)\n",
    "    cv2.line(img, (int(tdx), int(tdy)), (int(x3),int(y3)),(255,0,0),2)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5577b60-7f56-468e-8739-7fc2a4a12df7",
   "metadata": {},
   "source": [
    "## Real Time Video Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "017f722b-2830-4a57-af6a-64c7a670701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face Detection\n",
    "def detect_face(img):\n",
    "    boxes = get_bbox(img)\n",
    "    if len(boxes) > 0:\n",
    "        got_face = True\n",
    "        bbox = get_best_bbox(boxes, img.shape)\n",
    "    else:\n",
    "        got_face = False\n",
    "        bbox = []\n",
    "    return got_face, bbox\n",
    "\n",
    "# Crop Face Image by scale\n",
    "def crop_face(target_bbox, img, scale, row_adj_ratio=1.8, clm_adj_ratio=1):\n",
    "    bbox_nclm = target_bbox[2] - target_bbox[0]\n",
    "    bbox_nrow = target_bbox[3] - target_bbox[1]\n",
    "    clm_adj = (bbox_nclm * (scale - 1)) // 2\n",
    "    row_adj = (bbox_nrow * (scale - 1)) // 2\n",
    "    scaled_bbox = [int(max(target_bbox[0] - clm_adj_ratio*clm_adj,0)), int(max(target_bbox[1] - row_adj_ratio*row_adj, 0)),\n",
    "                   int(target_bbox[2] + (2-clm_adj_ratio)*clm_adj), int(target_bbox[3] + (2-row_adj_ratio)*row_adj)]\n",
    "    return scaled_bbox\n",
    "\n",
    "# Predict Euler\n",
    "def predict_eur(hpn_model, img, scaled_bbox, nbins=66, angle_range=99):\n",
    "    bin_index = np.array([idx for idx in range(nbins)], dtype=np.float32)\n",
    "    bin_degree = 2 * angle_range / nbins\n",
    "    face_img = img[scaled_bbox[1]:scaled_bbox[3], scaled_bbox[0]:scaled_bbox[2]]\n",
    "    # input_img = cv2.resize(face_img, (224,224))\n",
    "    input_img = cv2.resize(face_img, (224,224))\n",
    "    input_img = np.reshape(input_img, (-1,224,224,3)).astype(np.float32)\n",
    "    bin_pred = hpn_model(input_img)\n",
    "    pitch_angle = np.sum(softmax(bin_pred[\"Pitch\"]) * bin_index, 1) * bin_degree - angle_range\n",
    "    roll_angle = np.sum(softmax(bin_pred[\"Roll\"]) * bin_index, 1) * bin_degree - angle_range\n",
    "    return pitch_angle, roll_angle\n",
    "    \n",
    "# Predict Eye / Mouth\n",
    "eye = 0\n",
    "mouth = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b467dc",
   "metadata": {},
   "source": [
    "**Mouth and Eyes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7791f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de182d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf52b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "def are_eyes_closed(landmarks, image):\n",
    "    coords=[]\n",
    "    for i in (386, 362, 374, 263, 159, 33, 145, 133):\n",
    "        x2_loc=int(landmarks.landmark[i].x*image.shape[1])\n",
    "        y2_loc=int(landmarks.landmark[i].y*image.shape[0])\n",
    "        coords.append((x2_loc,y2_loc))\n",
    "        \n",
    "    sum=0\n",
    "    right_width = math.sqrt( (coords[1][0] - coords[3][0])**2 +\n",
    "                          (coords[1][1] - coords[3][1])**2   )\n",
    "    left_width = math.sqrt( (coords[5][0] - coords[7][0])**2 +\n",
    "                          (coords[5][1] - coords[7][1])**2   )\n",
    "    \n",
    "    # distance between two near points up and down\n",
    "    right_distance = math.sqrt( (coords[0][0] - coords[2][0])**2 +\n",
    "                              (coords[0][1] - coords[2][1])**2   )\n",
    "    left_distance = math.sqrt( (coords[4][0] - coords[6][0])**2 +\n",
    "                              (coords[4][1] - coords[6][1])**2   )\n",
    "    \n",
    "    ratio=0.15\n",
    "    if right_distance < right_width * ratio and left_distance < left_width * ratio:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cf09531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lip_height(landmarks, image):\n",
    "    coords=[]\n",
    "    for i in (0, 13, 14, 17, 37, 82, 87, 84, 267, 312, 317, 314):\n",
    "        x2_loc=int(landmarks.landmark[i].x*image.shape[1])\n",
    "        y2_loc=int(landmarks.landmark[i].y*image.shape[0])\n",
    "        coords.append((x2_loc,y2_loc))\n",
    "\n",
    "    sum_top=0\n",
    "    sum_bottom=0\n",
    "\n",
    "    for i in [0,4,8]:\n",
    "        # distance between two near points up and down\n",
    "        top_distance = math.sqrt( (coords[i][0] - coords[1+i][0])**2 +\n",
    "                              (coords[i][1] - coords[1+i][1])**2   )\n",
    "    for i in [2,6,10]:\n",
    "        # distance between two near points up and down\n",
    "        bottom_distance = math.sqrt( (coords[i][0] - coords[1+i][0])**2 +\n",
    "                              (coords[i][1] - coords[1+i][1])**2   )\n",
    "\n",
    "    sum_top += top_distance\n",
    "    sum_bottom += bottom_distance\n",
    "    return (sum_top / 3,sum_bottom / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a0f1061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mouth_height(landmarks, image):\n",
    "    coords=[]\n",
    "    for i in (0, 13, 14, 17, 37, 82, 87, 84, 267, 312, 317, 314):\n",
    "        x2_loc=int(landmarks.landmark[i].x*image.shape[1])\n",
    "#         print(x2_loc)\n",
    "        y2_loc=int(landmarks.landmark[i].y*image.shape[0])\n",
    "        coords.append((x2_loc,y2_loc))\n",
    "    \n",
    "    sum=0\n",
    "    for i in [1,5,9]:\n",
    "        # distance between two near points up and down\n",
    "        distance = math.sqrt( (coords[i][0] - coords[i+1][0])**2 + \n",
    "                              (coords[i][1] - coords[i+1][1])**2   )\n",
    "        sum += distance\n",
    "    return sum / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "399ac42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mouth_open(landmarks, image):\n",
    "    top_lip_height, bottom_lip_height=get_lip_height(landmarks, image)\n",
    "    mouth_height=get_mouth_height(landmarks, image)\n",
    "    \n",
    "    # if mouth is open more than lip height * ratio, return true.\n",
    "    ratio = 1.0\n",
    "    if mouth_height > (top_lip_height + bottom_lip_height) * ratio:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89e160cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_mesh = mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True, \n",
    "                                  min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "def Eyes_and_mouth(frame):\n",
    "    results = face_mesh.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    if results.multi_face_landmarks is not None:\n",
    "        closed_eyes=are_eyes_closed(results.multi_face_landmarks[0], frame)\n",
    "\n",
    "        open_mouth=check_mouth_open(results.multi_face_landmarks[0], frame)\n",
    "\n",
    "        return (True,closed_eyes, open_mouth)\n",
    "    else:\n",
    "        return (False, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ccc547",
   "metadata": {},
   "source": [
    "**Play the Game**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2f7dbf3-993b-4566-80e5-91c3a52fad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize Time Recording\n",
    "previous = time.time()\n",
    "\n",
    "# Parameters\n",
    "input_width = 320\n",
    "input_height = 240\n",
    "input_std = 128\n",
    "threshold = 0.5\n",
    "frame_count = 0\n",
    "frame_skip = 1\n",
    "skip = False\n",
    "\n",
    "# Initial Position\n",
    "calibration_frame_size = 5\n",
    "pitch_calibration = np.zeros(calibration_frame_size)\n",
    "roll_calibration = np.zeros(calibration_frame_size)\n",
    "init_pitch = 0\n",
    "init_roll = 10\n",
    "\n",
    "# Difference Threshold\n",
    "pitch_up_thres = 15\n",
    "pitch_down_thres = 15\n",
    "roll_left_thres = 17\n",
    "roll_right_thres = 17\n",
    "\n",
    "# Status Indicator\n",
    "stdby_mode = True\n",
    "play_mode = False\n",
    "pend_command = True\n",
    "mode_exec_time = 0\n",
    "mode_exec_thres = 2\n",
    "mouth_exec_time = 0\n",
    "mouth_exec_thres = 1\n",
    "command_exec_time = 0\n",
    "command_exec_thres = 0.5\n",
    "prev_command = None\n",
    "frame_count = 0\n",
    "\n",
    "# Initialize Browser\n",
    "# webbrowser.open_new('https://www.google.com/logos/2010/pacman10-i.html')\n",
    "webbrowser.open_new('https://www.freetetris.org/game.php')\n",
    "time.sleep(1)\n",
    "window_browser = gw.getActiveWindow()\n",
    "pyautogui.hotkey('winleft', 'right')\n",
    "window_active = False\n",
    "\n",
    "# Initialize Controls\n",
    "first_launch = True\n",
    "\n",
    "while True:\n",
    "    # Exit Condition\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "    # Start reading Image from Webcam\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    # Exit Condition if no image capture\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Preprocessing\n",
    "    frame = cv2.flip(frame,1)\n",
    "    \n",
    "    \n",
    "    # Update Current Mode\n",
    "    if stdby_mode and not play_mode:\n",
    "        cv2.putText(frame, 'Standby Mode', (545, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "    elif not stdby_mode and play_mode:\n",
    "        cv2.putText(frame, 'Play Mode', (569, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    # Update Status of Pending Command or under Latency\n",
    "    if pend_command:\n",
    "        cv2.putText(frame, 'Pending Command', (515, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "    elif not pend_command:\n",
    "        cv2.putText(frame, 'Breathing Time', (539, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    # Skip Frame\n",
    "    if frame_count % frame_skip == 0:\n",
    "        skip = False\n",
    "    else:\n",
    "        skip = True\n",
    "        \n",
    "    # Detect Face\n",
    "    face_ind, bbox = detect_face(frame)\n",
    "    \n",
    "    # Infer Head Pose and Eye/Mouth\n",
    "    if face_ind:\n",
    "        # Scale and Crop Face Image\n",
    "        if not skip:\n",
    "            face_bbox = crop_face(bbox, frame, 1.3)\n",
    "        cv2.putText(frame, 'Face Detected', (543, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "        cv2.rectangle(frame, (face_bbox[0], face_bbox[1]), (face_bbox[2], face_bbox[3]), (0, 0, 255), 2)\n",
    "\n",
    "        # Predict Euler\n",
    "        if not skip:\n",
    "            pitch_raw, roll_raw = predict_eur(hpn, frame, face_bbox)\n",
    "            success_EM, closed_eyes, open_mouth=Eyes_and_mouth(frame)\n",
    "        pitch = int(pitch_raw[0])\n",
    "        roll  = int(roll_raw[0])\n",
    "        cv2.putText(frame, 'Pitch : {0:d}'.format(pitch), (10, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,102,255), 1, cv2.LINE_AA)\n",
    "        cv2.putText(frame, 'Roll  : {0:d}'.format(roll), (10, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,102,255), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Print Neutral Position for Pitch and Roll\n",
    "        cv2.putText(frame, 'Neutral Pitch : {0:d}'.format(init_pitch), (10, 85), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,102,255), 1, cv2.LINE_AA)\n",
    "        cv2.putText(frame, 'Neutral Roll  : {0:d}'.format(init_roll), (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,102,255), 1, cv2.LINE_AA)\n",
    "        \n",
    "    else:\n",
    "        cv2.putText(frame, 'No Face Detected', (520, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    if not success_EM:\n",
    "        cv2.putText(frame, 'Mouth and Eyes detection failed', (520, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "            \n",
    "    # cv2.putText(frame, 'Action', (595, 455), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (51,255,0), 1, cv2.LINE_AA)\n",
    "    # cv2.putText(frame, 'Up', (616, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (51,255,0), 1, cv2.LINE_AA)\n",
    "    # cv2.putText(frame, 'Down', (600, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (51,255,0), 1, cv2.LINE_AA)\n",
    "    # cv2.putText(frame, 'Left', (608, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (51,255,0), 1, cv2.LINE_AA)\n",
    "    # cv2.putText(frame, 'Right', (601, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (51,255,0), 1, cv2.LINE_AA)\n",
    "    # cv2.putText(frame, 'Eye Close', (571, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (51,255,0), 1, cv2.LINE_AA)\n",
    "    # cv2.putText(frame, 'Eye Open', (574, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (51,255,0), 1, cv2.LINE_AA)\n",
    "    # cv2.putText(frame, 'Mouth Close', (556, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (51,255,0), 1, cv2.LINE_AA)\n",
    "    # cv2.putText(frame, 'Mouth Open', (558, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (51,255,0), 1, cv2.LINE_AA)\n",
    "    \n",
    "    # Execute Action\n",
    "    if face_ind and not stdby_mode and play_mode and pend_command:\n",
    "        if (roll-init_roll) > roll_right_thres:\n",
    "            command_exec_time = time.time()\n",
    "            prev_command = 'Right'\n",
    "            pend_command = False\n",
    "            pyautogui.press('right')\n",
    "        elif (roll-init_roll) < -roll_left_thres:\n",
    "            command_exec_time = time.time()\n",
    "            prev_command = 'Left'\n",
    "            pend_command = False\n",
    "            pyautogui.press('left')\n",
    "        elif (pitch-init_pitch) > pitch_up_thres:\n",
    "            command_exec_time = time.time()\n",
    "            prev_command = 'Up'\n",
    "            pend_command = False\n",
    "            pyautogui.press('up')\n",
    "        elif (pitch-init_pitch) < -pitch_down_thres:\n",
    "            command_exec_time = time.time()\n",
    "            prev_command = 'Down'\n",
    "            pend_command = False\n",
    "            pyautogui.press('down')\n",
    "        elif open_mouth:\n",
    "            cv2.putText(frame, 'Mouth Open...', (200, 455), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "            if mouth_exec_time == 0 and open_mouth:\n",
    "                mouth_exec_time = time.time()\n",
    "                cv2.putText(frame, 'Hold Time  : {0:0.2f}'.format(round(time.time()-mouth_exec_time,2)), (200, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "            elif mouth_exec_time > 0 and open_mouth and (time.time()-mouth_exec_time) >= mouth_exec_thres:\n",
    "                cv2.putText(frame, 'Hold Time  : {0:0.2f}'.format(round(time.time()-mouth_exec_time,2)), (200, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "                mouth_exec_time = 0\n",
    "                pyautogui.press('space')\n",
    "            elif mouth_exec_time > 0 and open_mouth and (time.time()-mouth_exec_time) < mouth_exec_thres:\n",
    "                cv2.putText(frame, 'Hold Time  : {0:0.2f}'.format(round(time.time()-mouth_exec_time,2)), (200, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "        elif mouth_exec_time > 0 and not open_mouth:\n",
    "            mouth_exec_time=0\n",
    "    \n",
    "        # Enter Play Mode, Standby Mode -> Play Mode\n",
    "    if face_ind and stdby_mode and not play_mode and pend_command:\n",
    "        # Trigger Play Mode\n",
    "        if closed_eyes:\n",
    "            cv2.putText(frame, 'Turning On...', (10, 455), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "        if mode_exec_time == 0 and closed_eyes:\n",
    "            mode_exec_time = time.time()\n",
    "            cv2.putText(frame, 'Hold Time  : {0:0.2f}'.format(round(time.time()-mode_exec_time,2)), (10, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "        elif mode_exec_time > 0 and closed_eyes and (time.time()-mode_exec_time) >= mode_exec_thres:\n",
    "            cv2.putText(frame, 'Hold Time  : {0:0.2f}'.format(round(time.time()-mode_exec_time,2)), (10, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "            stdby_mode=False\n",
    "            play_mode=True\n",
    "            mode_exec_time = 0\n",
    "            winsound.Beep(440, 500)\n",
    "            if first_launch:\n",
    "                pyautogui.press('space')\n",
    "                first_launch = False\n",
    "            else:\n",
    "                pyautogui.press('escape')\n",
    "        elif mode_exec_time > 0 and closed_eyes and (time.time()-mode_exec_time) < mode_exec_thres:\n",
    "            cv2.putText(frame, 'Hold Time  : {0:0.2f}'.format(round(time.time()-mode_exec_time,2)), (10, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "        elif mode_exec_time > 0 and not closed_eyes:\n",
    "            mode_exec_time=0\n",
    "        # elif mode_exec_time == 0 and (pitch-init_pitch) > -pitch_thres and frame_count % 10 == 0:\n",
    "        #     pitch_calibration = np.append(pitch_calibration[1:], pitch)\n",
    "        #     roll_calibration = np.append(roll_calibration[1:], roll)\n",
    "        #     init_pitch = int(np.mean(pitch_calibration))\n",
    "        #     init_roll = int(np.mean(roll_calibration))\n",
    "    \n",
    "    # Exit Play Mode, Play Mode -> Standby Mode\n",
    "    if face_ind and not stdby_mode and play_mode and pend_command:\n",
    "        if closed_eyes:\n",
    "            cv2.putText(frame, 'Turning Off...', (10, 455), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "        if mode_exec_time == 0 and closed_eyes:\n",
    "            mode_exec_time = time.time()\n",
    "            cv2.putText(frame, 'Hold Time  : {0:0.2f}'.format(round(time.time()-mode_exec_time,2)), (10, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "        elif mode_exec_time > 0 and closed_eyes and (time.time()-mode_exec_time) >= mode_exec_thres:\n",
    "            cv2.putText(frame, 'Hold Time  : {0:0.2f}'.format(round(time.time()-mode_exec_time,2)), (10, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "            stdby_mode=True\n",
    "            play_mode=False\n",
    "            mode_exec_time = 0\n",
    "            winsound.Beep(440, 500)\n",
    "            pyautogui.press('escape')\n",
    "        elif mode_exec_time > 0 and closed_eyes and (time.time()-mode_exec_time) < mode_exec_thres:\n",
    "            cv2.putText(frame, 'Hold Time  : {0:0.2f}'.format(round(time.time()-mode_exec_time,2)), (10, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "        elif mode_exec_time > 0 and not closed_eyes:\n",
    "            mode_exec_time=0\n",
    "    \n",
    "    # Breathing Period\n",
    "    if face_ind and not stdby_mode and play_mode and not pend_command:\n",
    "        if (time.time()-command_exec_time) >= command_exec_thres:\n",
    "            command_exec_time = 0\n",
    "            prev_command = None\n",
    "            pend_command = True\n",
    "        else:\n",
    "            cv2.putText(frame, 'Action', (595, 455), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "            cv2.putText(frame, f'{prev_command}', (608, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n",
    "        \n",
    "    \n",
    "    # Refresh FPS\n",
    "    if not skip:\n",
    "        new = time.time()\n",
    "        f = int(frame_skip/(new - previous))\n",
    "        previous = new \n",
    "    cv2.putText(frame, 'FPS  : {0:d}'.format(f), (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,0,0), 1, cv2.LINE_AA)\n",
    "    \n",
    "    # Showing Image Result\n",
    "    winname = \"Test\"\n",
    "    cv2.namedWindow(winname)       \n",
    "    # cv2.moveWindow(winname, 2000,300)\n",
    "    cv2.imshow(winname, frame)\n",
    "    \n",
    "    # Switching to application window\n",
    "    if not window_active:\n",
    "        window_active=True\n",
    "        window_browser.activate()\n",
    "    \n",
    "    frame_count += 1\n",
    "    \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
